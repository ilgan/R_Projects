Scraping and Sentiment with R
================
gvdr

![](https://i.ytimg.com/vi/8_5TCqHVEW8/maxresdefault.jpg)

We are starting in this lecture, and end in the next one.

The goal is to build a little collection of songs from our own preferred
artist. Let’s say, it’s *Straight Line Stitch* (they are great\!). A
little kicker for the
[morning](https://www.youtube.com/watch?v=4_5VAKdHMek).

## Packages

> Don’t be afraid of the dark you’re still held up by the stars

We are going to use a bunch of the usual packages:

```{r}
library(tidyverse)
library(magrittr)
library(purrr)
library(glue)
library(stringr)
```

and introduce a new one:

```{r}
library(rvest)
library(xml2)
```

which is meant explicitly to scrape stuff from a webpage. We are going
to use a couple more in the bonus section, if we get there.

## The lyrics

We are going to extract the lyrics from here:
<https://www.musixmatch.com/> . Chose it because it’s rather consistent,
and it’s from Bologna, Italy (yeah\!).

The webiste offers the first 15 lyrics up front. That will do for the
moment (and fixing that is not that easy). Let’s take a look
[here](https://www.musixmatch.com/artist/Straight-Line-Stitch#).

## Titles

First thing first, we would like to get a list of those title. Let’s see
how.

```{r}
url_titles <- "https://www.musixmatch.com/artist/Straight-Line-Stitch#"

page_title <- read_html(url_titles)
```

Now, what is this `page_title` object?

let’s see:

```{r}
page_title %>% View()
View(page_title)
```

    ## {xml_document}
    ## <html xmlns:og="http://ogp.me/ns#" class="artist-page-page">
    ## [1] <head prefix="og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# musixma ...
    ## [2] <body spellcheck="false" class="">\n  <div id="fb-root"></div>\n  <d ...

OK. It’s a document. Thanks. And it’s an XML document. That’s sort of
html. We’ll handle it with `xml2` and `rvest`. Let’s see a bit more of
that page.

```{r}
page_title %>% html_structure()
```


Wait, whaaaaaat?

![](https://media.giphy.com/media/ZkEXisGbMawMg/giphy.gif)

To the browser\! Look at that “class” tags: they are *css selectors*,
and we will use them as handles to navigate into the extremely complex
list that we get from a web page.

Sometimes, we can be lucky. For example, the css selector for the titles
are in the class “.title”. Let’s see.

```{r}
page_title %>%
  html_nodes(".title")
```



That’s still quite a mess: we have too much stuff, such as some links
(called “href”) and more text than we need. Let’s clean it up with
`html_text()`


css selector:

li.showPosition:nth-child(1) > div:nth-child(2) > div:nth-child(2) > div:nth-child(1) > h2:nth-child(1) > a:nth-child(1) > span:nth-child(1)

you need a DOT before the bname of the class. Ex: .title
```{r}
page_title %>%
  html_nodes(css = ".title") %>%
  html_text()
```


Wundebar\! Now we have 15 song titles. But we want the lyrics\! Let’s do
better.

```{r}
SLS_df <- data_frame(Band = "Straight Line Stitch",
                     Title = page_title %>%
                       html_nodes(".title") %>%
                       html_text(),
					 Link = glue('https://www.musixmatch.com/lyrics/{Band}/{Title}') %>% 
					 	str_replace_all(" ", "-"))

SLS_df$Link[1]
```

Now we are going to use a bit of string magic

```{r}
SLS_df %>%
  mutate(Bandc = str_replace_all(Band," ", "-"),
    Titlec = str_replace_all(Title," ", "-")) %>%
  glue_data("/lyrics/{Bandc}/{Titlec}")
```

 

It seems it works, let’s do stor the result into the dataframe:

```{r}
SLS_df %>%
  mutate(Bandc = str_replace_all(Band," ", "-"),
    Titlec = str_replace_all(Title," ", "-"),
    Link = glue("/lyrics/{Bandc}/{Titlec}")) %>%
  select(Link)
```


There is a better trick to do this job. If we look again at what we get
when we select the `.title` you may see that the *actual* link is there,
coded as `href`. Can we extract that? Yes we can\!

```{r}
page_title %>%
  html_nodes(".title") %>%
  html_attrs()
```

  
In particular, we want the element called `href`. Hey, we can get that
with `map`\!

```{r}
page_title %>%
  html_nodes(".title") %>%
  html_attrs() %>%
  map_chr("href")
```

```{r}
SLS_df %<>%
  mutate(Link = page_title %>%
  html_nodes(".title") %>%
  html_attrs() %>%
  map_chr("href"))
```

Cool, we don’t gain much in terms of line of code, but it will be
usefull later\!

## And `purrr`\!

Cool, now we want to put grab all lyrics. Let’s start with one at a
time. What is the url we want?

```{r}
url_song <- glue("https://www.musixmatch.com{SLS_df$Link[1]}")
```

And let’s grab the lyrics for that song. The content is marked by a css
selector called “p.mxm-lyrics\_\_content“. Whatever\!

```{r}
url_song %>%
  read_html() %>%
  html_nodes("p.mxm-lyrics__content") %>%
  html_text()
```
                                                                                                                                       

Ach, notice that it comes in different blocks: one for each section of
text, broken by the advertisment. Well, we can just `collapse()` them
together with `glue`. As we are doing this, let’s turn that flow into a
function:

```{r}
get_lyrics <- function(link){
  glue("https://www.musixmatch.com{link}") %>%
   read_html() %>%
   html_nodes("p.mxm-lyrics__content") %>%
   html_text() %>%
   collapse(sep = "\n") %>%
    return()
}
```

Let’s test it\!

```{r}
SLS_df$Link[3] %>%
  get_lyrics()
```

   
Now we can use purrr to map that function over our dataframe\!

```{r}
SLS_df %<>%
  mutate(Lyrics = map_chr(Link, get_lyrics))
```

Ok, here we were quite lucky, as all the links were right. In general we
may want to play safe, and use a `possibly` wrapper so not to have to
stop everything in case something bad happens.

### Bonus: sentiment analysis

The idea is to attribute to each word a score, expressing wether it’s
more negative and positive, and then to sum up. To do this, we are going
to use Julia Silge’s *Tidytext* library and a *vocabulary* of words for
which we have the scores (there are different options, we are using
“afinn”).

```{r}
library(tidytext)
afinn <- get_sentiments("afinn")
```

Now, a bit of massaging: we breaks the lyrics into their words, remove
the words that are considered not interesting (they are called “stop
words”), stitch the dataframe to the scoress from afinn, and do the math
for each song.

```{r}
SLS_df %>%
  unnest_tokens(word, Lyrics) %>% #split words
  anti_join(stop_words, by = "word") %>% #remove dull words
  inner_join(afinn, by = "word") %>% #stitch scores
  group_by(Title) %>% #and for each song
  summarise(Length = n(), #do the math
    Score = sum(score)/Length) %>%
  arrange(-Score)
```


So, what was the most positive song?

```{r}
SLS_df %>%
  filter(Title == "Promise Me") %$%
  Lyrics %>%
  glue()
```

## What about the rest?

We want to do it also for other artists. Best things is to turn some of
those scripts into functions. Let’s try with a *A Tribe Called Red* and
*Angel Haze* (I picked them ’cause they are great, and also because they
will show some limitations of the code I’m interested to tackle).

When we are about to do something over and over, it’s better to write
functions. So, let’s do it\!

### Challenge

Another singer you should, should, should listen to is *Militia Vox*.
Try to replicate our work with her lyrics. What’s the problem?

note: this is loosely inspired by Max Humber’s
[post](https://www.r-bloggers.com/fantasy-hockey-with-rvest-and-purrr/)
and David Laing’s post
[here](https://laingdk.github.io/kendrick-lamar-data-science/).



```{r}

library(tidytext)
```

